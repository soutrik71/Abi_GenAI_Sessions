{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/Abi_GenAI_Sessions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/Abi_GenAI_Sessions\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model import\n",
    "from src.encoder_decoder_transformer import build_transformer, casual_mask\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset,random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# HuggingFace libraries\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# typing\n",
    "from typing import Any\n",
    "\n",
    "# Library for progress bars in loops\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    N = 2\n",
    "    d_model = 256\n",
    "    d_ff = 2048\n",
    "    h = 8\n",
    "    dropout_rate = 0.1\n",
    "    batch_size = 32\n",
    "    num_epochs = 5\n",
    "    lr = 10**-4\n",
    "    seq_len = 350\n",
    "    lang_src = \"en\"\n",
    "    lang_tgt = \"it\"\n",
    "    model_folder = \"weights\"\n",
    "    model_basename = \"enmodel_\"\n",
    "    preload = None\n",
    "    tokenizer_file = \"tokenizer_{0}.json\"\n",
    "    token_folder = \"./tokenizers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for pair in ds:\n",
    "        yield pair[\"translation\"][lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Tokenization__\n",
    "- **Tokenization**: Each token in a sentence is mapped to a unique integer ID based on the vocabulary created during the training of the tokenizer.\n",
    "- **Vocabulary Mapping**: The integer ID represents a specific word in the vocabulary.\n",
    "- **Special Tokens in Transformers**:\n",
    "  - **[UNK]**: Identifies unknown words in a sequence.\n",
    "  - **[PAD]**: Used for padding to ensure all sequences in a batch have the same length; attention masks are used to ignore these tokens during training.\n",
    "  - **[SOS]**: Signals the Start of Sentence.\n",
    "  - **[EOS]**: Signals the End of Sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Tokenizer\n",
    "def build_tokenizer(config, ds, lang, force_download=True):\n",
    "    \"\"\"Function to build a tokenizer for a given language and dataset\"\"\"\n",
    "    # Crating a file path for the tokenizer\n",
    "    tokenizer_path = config.tokenizer_file.format(lang)\n",
    "\n",
    "    if force_download:\n",
    "        tokenizer = Tokenizer(\n",
    "            WordLevel(unk_token=\"[UNK]\")\n",
    "        )  # Initializing a new world-level tokenizer\n",
    "        tokenizer.pre_tokenizer = (\n",
    "            Whitespace()\n",
    "        )  # We will split the text into tokens based on whitespace\n",
    "\n",
    "        # Creating a trainer for the new tokenizer\n",
    "        trainer = WordLevelTrainer(\n",
    "            special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2\n",
    "        )  # Defining Word Level strategy and special tokens\n",
    "\n",
    "        # Training new tokenizer on sentences from the dataset and language specified\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(\n",
    "            str(tokenizer_path)\n",
    "        )  # Saving trained tokenizer to the file path specified at the beginning of the function\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(\n",
    "            str(tokenizer_path)\n",
    "        )  # If the tokenizer already exist, we load it\n",
    "    return tokenizer  # Returns the loaded tokenizer or the trained tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SoS] -> [tokens] -> [EoS] -> [pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    \"\"\" Dataset class for the bilingual dataset\"\"\"\n",
    "    def __init__(\n",
    "        self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        # Defining special tokens by using the target language tokenizer\n",
    "        self.sos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.eos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.pad_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64\n",
    "        )\n",
    "\n",
    "    # Total number of instances in the dataset (some pairs are larger than others)\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    # Using the index to retrive source and target texts\n",
    "    def __getitem__(self, index: Any) -> Any:\n",
    "        src_target_pair = self.ds[index]\n",
    "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
    "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
    "\n",
    "        # Tokenizing source and target texts\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Computing how many padding tokens need to be added to the tokenized texts\n",
    "        # Source tokens\n",
    "        enc_num_padding_tokens = (\n",
    "            self.seq_len - len(enc_input_tokens) - 2\n",
    "        )  # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
    "        # Target tokens\n",
    "        dec_num_padding_tokens = (\n",
    "            self.seq_len - len(dec_input_tokens) - 1\n",
    "        )  # Subtracting the '[SOS]' special token\n",
    "\n",
    "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
    "        # given the current sequence length limit (this will be defined in the config dictionary below)\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Building the encoder input tensor by combining several elements\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,  # inserting the '[SOS]' token\n",
    "                torch.tensor(\n",
    "                    enc_input_tokens, dtype=torch.int64\n",
    "                ),  # Inserting the tokenized source text\n",
    "                self.eos_token,  # Inserting the '[EOS]' token\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * enc_num_padding_tokens, dtype=torch.int64\n",
    "                ),  # Addind padding tokens\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Building the decoder input tensor by combining several elements\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,  # inserting the '[SOS]' token\n",
    "                torch.tensor(\n",
    "                    dec_input_tokens, dtype=torch.int64\n",
    "                ),  # Inserting the tokenized target text\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64\n",
    "                ),  # Addind padding tokens\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Creating a label tensor, the expected output for training the model\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(\n",
    "                    dec_input_tokens, dtype=torch.int64\n",
    "                ),  # Inserting the tokenized target text\n",
    "                self.eos_token,  # Inserting the '[EOS]' token\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64\n",
    "                ),  # Adding padding tokens\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,\n",
    "            \"decoder_input\": decoder_input,\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token)\n",
    "            .unsqueeze(0)\n",
    "            .unsqueeze(0)\n",
    "            .int(),\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token)\n",
    "            .unsqueeze(0)\n",
    "            .unsqueeze(0)\n",
    "            .int()\n",
    "            & casual_mask(decoder_input.size(0)),\n",
    "            \"label\": label,\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __DATASET PREPARATION__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the data using config src_lang and tgt_lang\n",
    "ds_raw = load_dataset(\n",
    "    \"opus_books\",\n",
    "    f\"{config.lang_src}-{config.lang_tgt}\",\n",
    "    split=\"train\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building or loading tokenizer for both the source and target languages\n",
    "tokenizer_src = build_tokenizer(config, ds_raw, config.lang_src)\n",
    "tokenizer_tgt = build_tokenizer(config, ds_raw, config.lang_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset for training and validation \n",
    "train_ds_size = int(0.3 * len(ds_raw)) # 90% for training\n",
    "val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n",
    "train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9699, 22633)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds_raw),len(val_ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data with the BilingualDataset class, which we will define below\n",
    "train_ds = BilingualDataset(\n",
    "    train_ds_raw,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    config.lang_src,\n",
    "    config.lang_tgt,\n",
    "    config.seq_len,\n",
    ")\n",
    "val_ds = BilingualDataset(\n",
    "    val_ds_raw,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    config.lang_src,\n",
    "    config.lang_tgt,\n",
    "    config.seq_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds_sample_op = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder_input', 'decoder_input', 'encoder_mask', 'decoder_mask', 'label', 'src_text', 'tgt_text'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainds_sample_op.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders are used to iterate over the dataset in batches during training and validation\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds, batch_size=config.batch_size, shuffle=True\n",
    ") \n",
    "val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input shape: torch.Size([32, 350])\n",
      "Decoder input shape: torch.Size([32, 350])\n",
      "Encoder mask shape: torch.Size([32, 1, 1, 350])\n",
      "Decoder mask shape: torch.Size([32, 1, 350, 350])\n",
      "Label shape: torch.Size([32, 350])\n",
      "Source text: ['The enigma then was explained: this affable and kind little widow was no great dame; but a dependant like myself.', \"Yashvin's face wore the expression it had when he was losing at cards.\", '\"Is she going by herself?\" asked the porter\\'s wife.', 'While still in the hall he heard her retreating footsteps, and knew that she had been waiting and listening for him, but had now gone back to the drawing-room.', 'The clock struck ten.', '\"Mr. Brocklehurst, I believe I intimated in the letter which I wrote to you three weeks ago, that this little girl has not quite the character and disposition I could wish: should you admit her into Lowood school, I should be glad if the superintendent and teachers were requested to keep a strict eye on her, and, above all, to guard against her worst fault, a tendency to deceit.', \"'I am so glad!' answered Dolly, smiling, but in a colder tone than she intended. 'I am very glad for your sake.\", 'When, on the evening of his return, he informed the steward of his intentions, the steward with evident pleasure agreed with that part of the plan which showed that all that had been done up to then was foolish and unprofitable.', \"'That is impossible!\", '\"I mean, on the contrary, to be busy.\"', \"Vronsky's mother, on hearing of the matter, was at first pleased, both because in her opinion nothing gave such finishing touches to a brilliant young man as an intrigue in the best Society, and also because this Anna Karenina, who had so taken her fancy and who had talked so much about her little son, was after all such as the Countess Vronsky expected all handsome and well-bred women to be.\", 'Some may wonder how it can happen that Agathocles, and his like, after infinite treacheries and cruelties, should live for long secure in his country, and defend himself from external enemies, and never be conspired against by his own citizens; seeing that many others, by means of cruelty, have never been able even in peaceful times to hold the state, still less in the doubtful times of war.', 'She was changing her dress.', 'But when I had wrought out some boards as above, I made large shelves, of the breadth of a foot and a half, one over another all along one side of my cave, to lay all my tools, nails and ironwork on; and, in a word, to separate everything at large into their places, that I might come easily at them.', 'Easter found snow still on the ground; but on Easter Monday a warm wind began to blow, the clouds gathered, and for three days and nights warm stormy rain poured down.', 'At one moment she was horrified at this indifference, and the next moment rejoiced at that which caused her indifference.', 'While this was doing, I was not altogether careless of my other affairs; for I had a great concern upon me for my little herd of goats: they were not only a ready supply to me on every occasion, and began to be sufficient for me, without the expense of powder and shot, but also without the fatigue of hunting after the wild ones; and I was loath to lose the advantage of them, and to have them all to nurse up over again.', \"'It is extraordinary how much he resembles the Public Prosecutor Sventitsky,' remarked one of the guests in French of the valet, while Vronsky, frowning, read his letter.\", \"'I wanted to tell you something long ago, Mama.\", 'Your words are such as ought not to be used: violent, unfeminine, and untrue.', '\"I don\\'t see it,\" said George, turning round.', 'To the young party also belonged the Court uniforms, which here and there ornamented the crowd.', 'This grieved me heartily; and now I saw, though too late, the folly of beginning a work before we count the cost, and before we judge rightly of our own strength to go through with it.', \"She suffers dreadfully; it's pitiful to see her, and everything in the house is topsy-turvy.\", 'Bessie, when she heard this narrative, sighed and said, \"Poor Miss Jane is to be pitied, too, Abbot.\"', 'Neither George nor I dared to turn round.', 'It was, especially, upon retiring to bed late in the night of the seventh or eighth day after the placing of the lady Madeline within the donjon, that I experienced the full power of such feelings.', 'She it was who had been to see Anna that morning, and they had been shopping together.', 'And then they do much good.', 'At that moment a child began to cry in another room, probably having tumbled down. Darya Alexandrovna listened, and her face softened suddenly.', 'Just two circumstances were not quite satisfactory, but they were drowned in the ocean of kind-hearted joviality which overflowed his heart.', \"'Hired labourers don't want to work well with good tools.\"]\n",
      "Target text: [\"Così l'enigma era spiegato. Quella piccola vedova, affidabile e buona, era non una gran signora, ma una dipendente come me.\", 'Sul viso di Jašvin c’era l’espressione che soleva avere quando perdeva al giuoco.', '— Parte sola? — domandò la portinaia.', 'Mentre era ancora nell’ingresso udì i passi di lei che si allontanavano. Capì che lo aspettava, che tendeva l’orecchio e che proprio allora era tornata nel salotto.', \"L'orologio suonò le dieci.\", \"— Credo, signor Bockelhurst, di avervi accennato nella mia lettera di tre settimane fa, che questa bimba non ha il carattere, nè le tendenze che avrei desiderato trovare in lei. Se dunque l'ammettete nella scuola di Lowood, domando che i capi e le maestre non la perdano d'occhio; la prego soprattutto di tenersi in guardia contro il suo più gran difetto; intendo parlare della sua tendenza alla menzogna.\", '— Come sono contenta! — disse, sorridendo, Dolly, involontariamente più fredda di quanto volesse. — Sono molto contenta per te.', 'Quando, la sera stessa in cui arrivò a casa, comunicò all’amministratore i suoi piani, l’amministratore con evidente soddisfazione, concordò con lui in quella parte del discorso in cui si dimostrava che quanto si era fatto finora era stato assurdo e poco conveniente.', '— Questo è impossibile!', '— Al contrario voglio essere operosa.', 'La madre di Vronskij, conosciuta la relazione di lui, in principio ne era stata contenta perché nulla, secondo lei, dava l’ultima finitura a un giovane brillante quanto una relazione nel gran mondo, e perché la Karenina, che tanto le era piaciuta, che tanto aveva parlato del proprio figlio, aveva finito con l’essere così come, secondo la contessa Vronskaja, doveva essere ogni bella donna del gran mondo.', \"Potrebbe alcuno dubitare donde nascessi che Agatocle et alcuno simile, dopo infiniti tradimenti e crudeltà, possé vivere lungamente sicuro nella sua patria e defendersi dalli inimici esterni, e da' sua cittadini non li fu mai conspirato contro; con ciò sia che molti altri, mediante la crudeltà non abbino, etiam ne' tempi pacifici, possuto mantenere lo stato, non che ne' tempi dubbiosi di guerra.\", 'Si stava cambiando.', 'Alcune altre assi che mi procurai nel modo dianzi indicato, mi giovarono a fare ampi scaffali della larghezza di un piede e mezzo collocati un sopra l’altro lungo una parete della mia grotta per annicchiarvi tutti i miei arnesi, chiodi e ferramenti, ed in una parola per tenere tutte le cose mie in tal conveniente distanza l’una dall’altra, da non durar fatica a trovarle quando ne aveva bisogno.', 'Ma due giorni dopo la settimana santa, si levò a un tratto un vento tiepido, le nuvole si addensarono, e per tre notti cadde una pioggia burrascosa e calda.', 'Un momento aveva orrore di quell’indifferenza, un momento si rallegrava di quello che ve l’aveva condotta.', 'Nel tempo delle indicate operazioni io non trasandava certamente gli altri miei affari, nè soprattutto quello del mio piccolo armento di capre, che non solo erano un eccellente pasto pel giornaliero mio vitto, e cominciavano a bastarmi senza costringermi a consumare le mie munizioni; ma mi dispensavano dalla fatica di andare alla caccia di animali salvatici. Ora mi rincresceva ugualmente a pensare di perdere gli utili che mi derivavano da questa greggia, e di dovermene allevar una di nuovo.', '— È sorprendente come assomigli al sostituto procuratore Sventickij — disse in francese, del cameriere, uno degli ospiti, mentre Vronskij leggeva la lettera, accigliato.', '— Ve lo volevo dire da tempo, maman.', 'Le vostre parole sono violente, indegne di una donna e false. Esse rivelano lo stato infelice della vostra mente e meriterebbero serii rimproveri.', '— Non la veggo — disse Giorgio, voltandosi.', 'Pure ai giovani appartenevano alcune divise di corte, che abbellivano qua e là la massa.', 'Oh quanto rammarico io ne sentii! Compresi allora, benchè troppo tardi, quanta sia la stoltezza di cominciare un lavoro prima di averne computata l’importanza e misurata rettamente la proporzione tra le nostre forze e il suo compimento.', 'Si tormenta molto ed è una pena guardarla, e poi tutto in casa va alla malora.', 'Bessie, dopo avere ascoltato questo, sospirò dicendo: — Povera signorina Jane; merita davvero compassione!', 'Nè Giorgio nè io osammo voltarci.', \"Fu una notte specialmente – la settima o l'ottava dopo che avevamo deposto lady Madeline nel sotterraneo – tardissimo, che provai tutta la potenza di quelle sensazioni.\", 'Era lei che era venuta la mattina e con lei Anna era andata a far delle spese.', 'Poi fanno molta beneficenza.', 'Nel frattempo, nella stanza accanto, probabilmente perché caduto, un bimbo si mise a gridare: Dar’ja Aleksandrovna tese l’orecchio, e il viso d’un tratto le si raddolcì.', 'Aveva solo due ragioni di malcontento; ma entrambe si sperdevano nel mare di benevola allegria che fluttuava nell’animo suo.', '— Già, ma i lavoratori non vogliono lavorare bene e con strumenti buoni.']\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataloader:\n",
    "    print(\"Encoder input shape:\", data[\"encoder_input\"].shape)\n",
    "    print(\"Decoder input shape:\", data[\"decoder_input\"].shape)\n",
    "    print(\"Encoder mask shape:\", data[\"encoder_mask\"].shape)\n",
    "    print(\"Decoder mask shape:\", data[\"decoder_mask\"].shape)\n",
    "    print(\"Label shape:\", data[\"label\"].shape)\n",
    "    print(\"Source text:\", data[\"src_text\"])\n",
    "    print(\"Target text:\", data[\"tgt_text\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    \"\"\"Function to build the transformer model\"\"\"\n",
    "    # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n",
    "    model = build_transformer(\n",
    "        src_vocab_size=vocab_src_len,\n",
    "        tgt_vocab_size=vocab_tgt_len,\n",
    "        src_seq_len=config.seq_len,\n",
    "        tgt_seq_len=config.seq_len,\n",
    "        d_model=config.d_model,\n",
    "        N=config.N,\n",
    "        d_ff=config.d_ff,\n",
    "        h=config.h,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to obtain the most probable next token\n",
    "def greedy_decode(\n",
    "    model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device\n",
    "):\n",
    "    \"\"\"Function to generate a sequence of tokens using the greedy decoding algorithm\n",
    "    Args:\n",
    "    model: Transformer model\n",
    "    source: Tensor with the source sequence\n",
    "    source_mask: Mask for the source sequence\n",
    "    tokenizer_src: Source language tokenizer\n",
    "    tokenizer_tgt: Target language tokenizer\n",
    "    max_len: Maximum length of the output sequence\n",
    "    device: Device to run the computations\n",
    "\n",
    "    Returns:\n",
    "    Tensor with the sequence of tokens generated by the decoder\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # This is same as the inference testing loop that we created in the model testing section\n",
    "\n",
    "    # Retrieving the indices from the start and end of sequences of the target tokens\n",
    "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
    "    eos_idx = tokenizer_tgt.token_to_id(\"[EOS]\")\n",
    "\n",
    "    # Computing the output of the encoder for the source sequence\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initializing the decoder input with the Start of Sentence token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    # Looping until the 'max_len', maximum length, is reached\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # Building a mask for the decoder input\n",
    "        decoder_mask = (\n",
    "            casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "        )\n",
    "\n",
    "        # Calculating the output of the decoder\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # Applying the projection layer to get the probabilities for the next token\n",
    "        prob = model.project(out[:, -1])\n",
    "\n",
    "        # Selecting token with the highest probability\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                decoder_input,\n",
    "                torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # If the next token is an End of Sentence token, we finish the loop\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)  # Sequence of tokens generated by the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(\n",
    "    model,\n",
    "    validation_ds,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    print_msg,\n",
    "    num_examples=2,\n",
    "):\n",
    "    \"\"\" Function to run the validation loop for the model\n",
    "    Args:\n",
    "    model: Transformer model\n",
    "    validation_ds: Validation dataset\n",
    "    tokenizer_src: Source language tokenizer\n",
    "    tokenizer_tgt: Target language tokenizer\n",
    "    max_len: Maximum length of the output sequence\n",
    "    device: Device to run the computations\n",
    "    print_msg: Function to print messages\n",
    "    num_examples: Number of examples to process\n",
    "    \"\"\"\n",
    "    model.eval()  # Setting model to evaluation mode\n",
    "    count = (\n",
    "        0  # Initializing counter to keep track of how many examples have been processed\n",
    "    )\n",
    "\n",
    "    # Creating evaluation loop\n",
    "    with torch.no_grad():  # Ensuring that no gradients are computed during this process\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "            # Ensuring that the batch_size of the validation set is 1\n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation.\"\n",
    "\n",
    "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "            model_out = greedy_decode(\n",
    "                model,\n",
    "                encoder_input,\n",
    "                encoder_mask,\n",
    "                tokenizer_src,\n",
    "                tokenizer_tgt,\n",
    "                max_len,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "            # Retrieving source and target texts from the batch\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]  # True translation\n",
    "            model_out_text = tokenizer_tgt.decode(\n",
    "                model_out.detach().cpu().numpy()\n",
    "            )  # Decoded, human-readable model output\n",
    "\n",
    "            print_msg(f\"SOURCE: {source_text}\")\n",
    "            print_msg(f\"TARGET: {target_text}\")\n",
    "            print_msg(f\"PREDICTED: {model_out_text}\")\n",
    "\n",
    "            # After two examples, we break the loop\n",
    "            if count == num_examples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt):\n",
    "    \"\"\" Function to train the Transformer model\n",
    "    Args:\n",
    "    config: Configuration dictionary\n",
    "    train_dataloader: Training dataloader\n",
    "    val_dataloader: Validation dataloader\n",
    "    tokenizer_src: Source language tokenizer\n",
    "    tokenizer_tgt: Target language tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setting up device to run on GPU to train faster\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device {device}\")\n",
    "\n",
    "    # Initializing model on the GPU using the 'get_model' function\n",
    "    model = get_model(\n",
    "        config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()\n",
    "    ).to(device)\n",
    "\n",
    "    # Setting up the Adam optimizer with the specified learning rate from the '\n",
    "    # config' dictionary plus an epsilon value\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, eps=1e-9)\n",
    "\n",
    "    # Initializing epoch and global step variables\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "    # Initializing CrossEntropyLoss function for training\n",
    "    # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
    "    # We also apply label_smoothing to prevent overfitting\n",
    "    loss_fn = nn.CrossEntropyLoss(\n",
    "        ignore_index=tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    # Initializing training loop\n",
    "\n",
    "    # Iterating over each epoch from the 'initial_epoch' variable up to\n",
    "    # the number of epochs informed in the config\n",
    "    for epoch in range(initial_epoch, config.num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(epoch)\n",
    "        model.train()\n",
    "        # Initializing an iterator over the training dataloader\n",
    "        # We also use tqdm to display a progress bar\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing epoch {epoch:02d}\")\n",
    "\n",
    "        # For each batch...\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            # Loading input data and masks onto the GPU\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)\n",
    "            decoder_input = batch[\"decoder_input\"].to(device)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "            decoder_mask = batch[\"decoder_mask\"].to(device)\n",
    "\n",
    "            # Running tensors through the Transformer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(\n",
    "                encoder_output, encoder_mask, decoder_input, decoder_mask\n",
    "            )\n",
    "            proj_output = model.project(decoder_output)\n",
    "\n",
    "            # Loading the target labels onto the GPU\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            # Computing loss between model's output and true labels\n",
    "            loss = loss_fn(\n",
    "                proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1)\n",
    "            )\n",
    "\n",
    "            # Updating progress bar\n",
    "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Performing backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters based on the gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clearing the gradients to prepare for the next batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1  # Updating global step count\n",
    "\n",
    "        # We run the 'run_validation' function at the end of each epoch\n",
    "        # to evaluate model performance\n",
    "        run_validation(\n",
    "            model,\n",
    "            val_dataloader,\n",
    "            tokenizer_src,\n",
    "            tokenizer_tgt,\n",
    "            config.seq_len,\n",
    "            device,\n",
    "            lambda msg: batch_iterator.write(msg),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 00: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 304/304 [22:41<00:00,  4.48s/it, loss=6.810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: \"Not to advertise: and to trust this quest of a situation to me. I'll find you one in time.\"\n",
      "TARGET: — Di non fare annunzi nei giornali, di affidare a me l'incarico di trovarvi una situazione; a suo tempo ve ne procurerò una.\n",
      "PREDICTED: — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —\n",
      "SOURCE: All these last days Dolly had been alone with her children.\n",
      "TARGET: Tutti quei giorni Dolly era stata sola coi bambini.\n",
      "PREDICTED: — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 01: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 304/304 [22:40<00:00,  4.48s/it, loss=6.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: At that hour most of the others were sewing likewise; but one class still stood round Miss Scatcherd's chair reading, and as all was quiet, the subject of their lessons could be heard, together with the manner in which each girl acquitted herself, and the animadversions or commendations of Miss Scatcherd on the performance.\n",
      "TARGET: Quasi tutte cucivano in quell'ora, eccetto alcune alunne che leggevano a voce alta attorno alla sedia della signorina Scatcherd.\n",
      "PREDICTED: — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —\n",
      "SOURCE: By nothing! – The devil and sin? – And how do I explain evil?...\n",
      "TARGET: Il demone e il peccato? E con che cosa spiegavo il male?...\n",
      "PREDICTED: — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 02: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 304/304 [22:56<00:00,  4.53s/it, loss=6.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: He has again and again explained that it is not himself, but his office he wishes to mate. He has told me I am formed for labour--not for love: which is true, no doubt.\n",
      "TARGET: Mi ha ripetuto che non era per sé che prendeva moglie, ma per adempiere la sua missione, che ero adattata al lavoro e non all'amore.\n",
      "PREDICTED: — E non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non .\n",
      "SOURCE: If we should wish to retreat, how ought we to pursue?\" And he would set forth to them, as he went, all the chances that could befall an army; he would listen to their opinion and state his, confirming it with reasons, so that by these continual discussions there could never arise, in time of war, any unexpected circumstances that he could not deal with.\n",
      "TARGET: come si potrebbe ire, servando li ordini, a trovarli? se noi volessimo ritirarci, come aremmo a fare? se loro si ritirassino, come aremmo a seguirli? - E proponeva loro, andando, tutti e' casi che in uno esercito possono occorrere; intendeva la opinione loro, diceva la sua, corroboravala con le ragioni: tal che, per queste continue cogitazioni, non posseva mai, guidando li eserciti, nascere accidente alcuno, che lui non avessi el remedio.\n",
      "PREDICTED: — E che non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non , ma non non non non non non non non non non non non non non non , e non non non .\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 03: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 304/304 [23:22<00:00,  4.61s/it, loss=6.120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: The hostess was a short, fair, round-faced woman, beaming with smiles and dimples.\n",
      "TARGET: La padrona era una donna dal viso tondo, bionda e non alta, tutta splendente di fossette e sorrisi.\n",
      "PREDICTED: Il suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo ’ era .\n",
      "SOURCE: She is a splendid woman.\n",
      "TARGET: È una carissima donna.\n",
      "PREDICTED: Ma non era un ’ era .\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 04: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 304/304 [22:32<00:00,  4.45s/it, loss=6.097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: \"While something in me,\" he went on, \"is acutely sensible to her charms, something else is as deeply impressed with her defects: they are such that she could sympathise in nothing I aspired to--co-operate in nothing I undertook.\n",
      "TARGET: — Se vi è qualcosa in me, — riprese, — che subisce fascino della sua attrattiva, qualcos'altro invece è urtato dai suoi difetti; ella non capirebbe aspirazioni, non potrebbe aiutarmi nelle mie imprese,\n",
      "PREDICTED: — Non è un ’ è un ’ è un ’ è un ’ è di , — disse che non è un ’ è un ’ è di .\n",
      "SOURCE: A Russian nursemaid was feeding the child and evidently herself eating also.\n",
      "TARGET: Dava da mangiare alla bambina, e visibilmente mangiava lei stessa insieme alla piccola, una ragazza russa che faceva il servizio nella camera della bambina.\n",
      "PREDICTED: Il suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo .\n"
     ]
    }
   ],
   "source": [
    "train_model(config, train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "* https://medium.com/ai-in-plain-english/building-and-training-a-transformer-from-scratch-fdbf3db00df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
