{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 08:21:34.308851: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 08:21:34.308905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 08:21:34.309804: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 08:21:34.316248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 08:21:35.143452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog\n"
     ]
    }
   ],
   "source": [
    "print(dataset['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization of models in 4bit NF for Qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloom-3b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['transformer.word_embeddings.weight', 'transformer.word_embeddings_layernorm.weight', 'transformer.word_embeddings_layernorm.bias', 'transformer.h.0.input_layernorm.weight', 'transformer.h.0.input_layernorm.bias', 'transformer.h.0.self_attention.query_key_value.weight', 'transformer.h.0.self_attention.query_key_value.bias', 'transformer.h.0.self_attention.query_key_value.weight.absmax', 'transformer.h.0.self_attention.query_key_value.weight.quant_map', 'transformer.h.0.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.0.self_attention.dense.weight', 'transformer.h.0.self_attention.dense.bias', 'transformer.h.0.self_attention.dense.weight.absmax', 'transformer.h.0.self_attention.dense.weight.quant_map', 'transformer.h.0.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.0.post_attention_layernorm.weight', 'transformer.h.0.post_attention_layernorm.bias', 'transformer.h.0.mlp.dense_h_to_4h.weight', 'transformer.h.0.mlp.dense_h_to_4h.bias', 'transformer.h.0.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.0.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.0.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.0.mlp.dense_4h_to_h.weight', 'transformer.h.0.mlp.dense_4h_to_h.bias', 'transformer.h.0.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.0.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.0.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.1.input_layernorm.weight', 'transformer.h.1.input_layernorm.bias', 'transformer.h.1.self_attention.query_key_value.weight', 'transformer.h.1.self_attention.query_key_value.bias', 'transformer.h.1.self_attention.query_key_value.weight.absmax', 'transformer.h.1.self_attention.query_key_value.weight.quant_map', 'transformer.h.1.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.1.self_attention.dense.weight', 'transformer.h.1.self_attention.dense.bias', 'transformer.h.1.self_attention.dense.weight.absmax', 'transformer.h.1.self_attention.dense.weight.quant_map', 'transformer.h.1.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.1.post_attention_layernorm.weight', 'transformer.h.1.post_attention_layernorm.bias', 'transformer.h.1.mlp.dense_h_to_4h.weight', 'transformer.h.1.mlp.dense_h_to_4h.bias', 'transformer.h.1.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.1.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.1.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.1.mlp.dense_4h_to_h.weight', 'transformer.h.1.mlp.dense_4h_to_h.bias', 'transformer.h.1.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.1.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.1.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.2.input_layernorm.weight', 'transformer.h.2.input_layernorm.bias', 'transformer.h.2.self_attention.query_key_value.weight', 'transformer.h.2.self_attention.query_key_value.bias', 'transformer.h.2.self_attention.query_key_value.weight.absmax', 'transformer.h.2.self_attention.query_key_value.weight.quant_map', 'transformer.h.2.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.2.self_attention.dense.weight', 'transformer.h.2.self_attention.dense.bias', 'transformer.h.2.self_attention.dense.weight.absmax', 'transformer.h.2.self_attention.dense.weight.quant_map', 'transformer.h.2.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.2.post_attention_layernorm.weight', 'transformer.h.2.post_attention_layernorm.bias', 'transformer.h.2.mlp.dense_h_to_4h.weight', 'transformer.h.2.mlp.dense_h_to_4h.bias', 'transformer.h.2.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.2.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.2.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.2.mlp.dense_4h_to_h.weight', 'transformer.h.2.mlp.dense_4h_to_h.bias', 'transformer.h.2.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.2.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.2.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.3.input_layernorm.weight', 'transformer.h.3.input_layernorm.bias', 'transformer.h.3.self_attention.query_key_value.weight', 'transformer.h.3.self_attention.query_key_value.bias', 'transformer.h.3.self_attention.query_key_value.weight.absmax', 'transformer.h.3.self_attention.query_key_value.weight.quant_map', 'transformer.h.3.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.3.self_attention.dense.weight', 'transformer.h.3.self_attention.dense.bias', 'transformer.h.3.self_attention.dense.weight.absmax', 'transformer.h.3.self_attention.dense.weight.quant_map', 'transformer.h.3.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.3.post_attention_layernorm.weight', 'transformer.h.3.post_attention_layernorm.bias', 'transformer.h.3.mlp.dense_h_to_4h.weight', 'transformer.h.3.mlp.dense_h_to_4h.bias', 'transformer.h.3.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.3.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.3.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.3.mlp.dense_4h_to_h.weight', 'transformer.h.3.mlp.dense_4h_to_h.bias', 'transformer.h.3.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.3.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.3.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.4.input_layernorm.weight', 'transformer.h.4.input_layernorm.bias', 'transformer.h.4.self_attention.query_key_value.weight', 'transformer.h.4.self_attention.query_key_value.bias', 'transformer.h.4.self_attention.query_key_value.weight.absmax', 'transformer.h.4.self_attention.query_key_value.weight.quant_map', 'transformer.h.4.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.4.self_attention.dense.weight', 'transformer.h.4.self_attention.dense.bias', 'transformer.h.4.self_attention.dense.weight.absmax', 'transformer.h.4.self_attention.dense.weight.quant_map', 'transformer.h.4.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.4.post_attention_layernorm.weight', 'transformer.h.4.post_attention_layernorm.bias', 'transformer.h.4.mlp.dense_h_to_4h.weight', 'transformer.h.4.mlp.dense_h_to_4h.bias', 'transformer.h.4.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.4.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.4.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.4.mlp.dense_4h_to_h.weight', 'transformer.h.4.mlp.dense_4h_to_h.bias', 'transformer.h.4.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.4.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.4.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.5.input_layernorm.weight', 'transformer.h.5.input_layernorm.bias', 'transformer.h.5.self_attention.query_key_value.weight', 'transformer.h.5.self_attention.query_key_value.bias', 'transformer.h.5.self_attention.query_key_value.weight.absmax', 'transformer.h.5.self_attention.query_key_value.weight.quant_map', 'transformer.h.5.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.5.self_attention.dense.weight', 'transformer.h.5.self_attention.dense.bias', 'transformer.h.5.self_attention.dense.weight.absmax', 'transformer.h.5.self_attention.dense.weight.quant_map', 'transformer.h.5.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.5.post_attention_layernorm.weight', 'transformer.h.5.post_attention_layernorm.bias', 'transformer.h.5.mlp.dense_h_to_4h.weight', 'transformer.h.5.mlp.dense_h_to_4h.bias', 'transformer.h.5.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.5.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.5.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.5.mlp.dense_4h_to_h.weight', 'transformer.h.5.mlp.dense_4h_to_h.bias', 'transformer.h.5.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.5.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.5.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.6.input_layernorm.weight', 'transformer.h.6.input_layernorm.bias', 'transformer.h.6.self_attention.query_key_value.weight', 'transformer.h.6.self_attention.query_key_value.bias', 'transformer.h.6.self_attention.query_key_value.weight.absmax', 'transformer.h.6.self_attention.query_key_value.weight.quant_map', 'transformer.h.6.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.6.self_attention.dense.weight', 'transformer.h.6.self_attention.dense.bias', 'transformer.h.6.self_attention.dense.weight.absmax', 'transformer.h.6.self_attention.dense.weight.quant_map', 'transformer.h.6.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.6.post_attention_layernorm.weight', 'transformer.h.6.post_attention_layernorm.bias', 'transformer.h.6.mlp.dense_h_to_4h.weight', 'transformer.h.6.mlp.dense_h_to_4h.bias', 'transformer.h.6.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.6.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.6.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.6.mlp.dense_4h_to_h.weight', 'transformer.h.6.mlp.dense_4h_to_h.bias', 'transformer.h.6.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.6.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.6.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.7.input_layernorm.weight', 'transformer.h.7.input_layernorm.bias', 'transformer.h.7.self_attention.query_key_value.weight', 'transformer.h.7.self_attention.query_key_value.bias', 'transformer.h.7.self_attention.query_key_value.weight.absmax', 'transformer.h.7.self_attention.query_key_value.weight.quant_map', 'transformer.h.7.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.7.self_attention.dense.weight', 'transformer.h.7.self_attention.dense.bias', 'transformer.h.7.self_attention.dense.weight.absmax', 'transformer.h.7.self_attention.dense.weight.quant_map', 'transformer.h.7.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.7.post_attention_layernorm.weight', 'transformer.h.7.post_attention_layernorm.bias', 'transformer.h.7.mlp.dense_h_to_4h.weight', 'transformer.h.7.mlp.dense_h_to_4h.bias', 'transformer.h.7.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.7.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.7.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.7.mlp.dense_4h_to_h.weight', 'transformer.h.7.mlp.dense_4h_to_h.bias', 'transformer.h.7.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.7.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.7.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.8.input_layernorm.weight', 'transformer.h.8.input_layernorm.bias', 'transformer.h.8.self_attention.query_key_value.weight', 'transformer.h.8.self_attention.query_key_value.bias', 'transformer.h.8.self_attention.query_key_value.weight.absmax', 'transformer.h.8.self_attention.query_key_value.weight.quant_map', 'transformer.h.8.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.8.self_attention.dense.weight', 'transformer.h.8.self_attention.dense.bias', 'transformer.h.8.self_attention.dense.weight.absmax', 'transformer.h.8.self_attention.dense.weight.quant_map', 'transformer.h.8.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.8.post_attention_layernorm.weight', 'transformer.h.8.post_attention_layernorm.bias', 'transformer.h.8.mlp.dense_h_to_4h.weight', 'transformer.h.8.mlp.dense_h_to_4h.bias', 'transformer.h.8.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.8.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.8.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.8.mlp.dense_4h_to_h.weight', 'transformer.h.8.mlp.dense_4h_to_h.bias', 'transformer.h.8.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.8.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.8.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.9.input_layernorm.weight', 'transformer.h.9.input_layernorm.bias', 'transformer.h.9.self_attention.query_key_value.weight', 'transformer.h.9.self_attention.query_key_value.bias', 'transformer.h.9.self_attention.query_key_value.weight.absmax', 'transformer.h.9.self_attention.query_key_value.weight.quant_map', 'transformer.h.9.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.9.self_attention.dense.weight', 'transformer.h.9.self_attention.dense.bias', 'transformer.h.9.self_attention.dense.weight.absmax', 'transformer.h.9.self_attention.dense.weight.quant_map', 'transformer.h.9.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.9.post_attention_layernorm.weight', 'transformer.h.9.post_attention_layernorm.bias', 'transformer.h.9.mlp.dense_h_to_4h.weight', 'transformer.h.9.mlp.dense_h_to_4h.bias', 'transformer.h.9.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.9.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.9.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.9.mlp.dense_4h_to_h.weight', 'transformer.h.9.mlp.dense_4h_to_h.bias', 'transformer.h.9.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.9.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.9.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.10.input_layernorm.weight', 'transformer.h.10.input_layernorm.bias', 'transformer.h.10.self_attention.query_key_value.weight', 'transformer.h.10.self_attention.query_key_value.bias', 'transformer.h.10.self_attention.query_key_value.weight.absmax', 'transformer.h.10.self_attention.query_key_value.weight.quant_map', 'transformer.h.10.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.10.self_attention.dense.weight', 'transformer.h.10.self_attention.dense.bias', 'transformer.h.10.self_attention.dense.weight.absmax', 'transformer.h.10.self_attention.dense.weight.quant_map', 'transformer.h.10.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.10.post_attention_layernorm.weight', 'transformer.h.10.post_attention_layernorm.bias', 'transformer.h.10.mlp.dense_h_to_4h.weight', 'transformer.h.10.mlp.dense_h_to_4h.bias', 'transformer.h.10.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.10.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.10.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.10.mlp.dense_4h_to_h.weight', 'transformer.h.10.mlp.dense_4h_to_h.bias', 'transformer.h.10.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.10.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.10.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.11.input_layernorm.weight', 'transformer.h.11.input_layernorm.bias', 'transformer.h.11.self_attention.query_key_value.weight', 'transformer.h.11.self_attention.query_key_value.bias', 'transformer.h.11.self_attention.query_key_value.weight.absmax', 'transformer.h.11.self_attention.query_key_value.weight.quant_map', 'transformer.h.11.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.11.self_attention.dense.weight', 'transformer.h.11.self_attention.dense.bias', 'transformer.h.11.self_attention.dense.weight.absmax', 'transformer.h.11.self_attention.dense.weight.quant_map', 'transformer.h.11.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.11.post_attention_layernorm.weight', 'transformer.h.11.post_attention_layernorm.bias', 'transformer.h.11.mlp.dense_h_to_4h.weight', 'transformer.h.11.mlp.dense_h_to_4h.bias', 'transformer.h.11.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.11.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.11.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.11.mlp.dense_4h_to_h.weight', 'transformer.h.11.mlp.dense_4h_to_h.bias', 'transformer.h.11.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.11.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.11.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.12.input_layernorm.weight', 'transformer.h.12.input_layernorm.bias', 'transformer.h.12.self_attention.query_key_value.weight', 'transformer.h.12.self_attention.query_key_value.bias', 'transformer.h.12.self_attention.query_key_value.weight.absmax', 'transformer.h.12.self_attention.query_key_value.weight.quant_map', 'transformer.h.12.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.12.self_attention.dense.weight', 'transformer.h.12.self_attention.dense.bias', 'transformer.h.12.self_attention.dense.weight.absmax', 'transformer.h.12.self_attention.dense.weight.quant_map', 'transformer.h.12.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.12.post_attention_layernorm.weight', 'transformer.h.12.post_attention_layernorm.bias', 'transformer.h.12.mlp.dense_h_to_4h.weight', 'transformer.h.12.mlp.dense_h_to_4h.bias', 'transformer.h.12.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.12.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.12.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.12.mlp.dense_4h_to_h.weight', 'transformer.h.12.mlp.dense_4h_to_h.bias', 'transformer.h.12.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.12.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.12.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.13.input_layernorm.weight', 'transformer.h.13.input_layernorm.bias', 'transformer.h.13.self_attention.query_key_value.weight', 'transformer.h.13.self_attention.query_key_value.bias', 'transformer.h.13.self_attention.query_key_value.weight.absmax', 'transformer.h.13.self_attention.query_key_value.weight.quant_map', 'transformer.h.13.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.13.self_attention.dense.weight', 'transformer.h.13.self_attention.dense.bias', 'transformer.h.13.self_attention.dense.weight.absmax', 'transformer.h.13.self_attention.dense.weight.quant_map', 'transformer.h.13.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.13.post_attention_layernorm.weight', 'transformer.h.13.post_attention_layernorm.bias', 'transformer.h.13.mlp.dense_h_to_4h.weight', 'transformer.h.13.mlp.dense_h_to_4h.bias', 'transformer.h.13.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.13.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.13.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.13.mlp.dense_4h_to_h.weight', 'transformer.h.13.mlp.dense_4h_to_h.bias', 'transformer.h.13.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.13.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.13.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.14.input_layernorm.weight', 'transformer.h.14.input_layernorm.bias', 'transformer.h.14.self_attention.query_key_value.weight', 'transformer.h.14.self_attention.query_key_value.bias', 'transformer.h.14.self_attention.query_key_value.weight.absmax', 'transformer.h.14.self_attention.query_key_value.weight.quant_map', 'transformer.h.14.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.14.self_attention.dense.weight', 'transformer.h.14.self_attention.dense.bias', 'transformer.h.14.self_attention.dense.weight.absmax', 'transformer.h.14.self_attention.dense.weight.quant_map', 'transformer.h.14.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.14.post_attention_layernorm.weight', 'transformer.h.14.post_attention_layernorm.bias', 'transformer.h.14.mlp.dense_h_to_4h.weight', 'transformer.h.14.mlp.dense_h_to_4h.bias', 'transformer.h.14.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.14.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.14.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.14.mlp.dense_4h_to_h.weight', 'transformer.h.14.mlp.dense_4h_to_h.bias', 'transformer.h.14.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.14.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.14.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.15.input_layernorm.weight', 'transformer.h.15.input_layernorm.bias', 'transformer.h.15.self_attention.query_key_value.weight', 'transformer.h.15.self_attention.query_key_value.bias', 'transformer.h.15.self_attention.query_key_value.weight.absmax', 'transformer.h.15.self_attention.query_key_value.weight.quant_map', 'transformer.h.15.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.15.self_attention.dense.weight', 'transformer.h.15.self_attention.dense.bias', 'transformer.h.15.self_attention.dense.weight.absmax', 'transformer.h.15.self_attention.dense.weight.quant_map', 'transformer.h.15.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.15.post_attention_layernorm.weight', 'transformer.h.15.post_attention_layernorm.bias', 'transformer.h.15.mlp.dense_h_to_4h.weight', 'transformer.h.15.mlp.dense_h_to_4h.bias', 'transformer.h.15.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.15.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.15.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.15.mlp.dense_4h_to_h.weight', 'transformer.h.15.mlp.dense_4h_to_h.bias', 'transformer.h.15.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.15.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.15.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.16.input_layernorm.weight', 'transformer.h.16.input_layernorm.bias', 'transformer.h.16.self_attention.query_key_value.weight', 'transformer.h.16.self_attention.query_key_value.bias', 'transformer.h.16.self_attention.query_key_value.weight.absmax', 'transformer.h.16.self_attention.query_key_value.weight.quant_map', 'transformer.h.16.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.16.self_attention.dense.weight', 'transformer.h.16.self_attention.dense.bias', 'transformer.h.16.self_attention.dense.weight.absmax', 'transformer.h.16.self_attention.dense.weight.quant_map', 'transformer.h.16.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.16.post_attention_layernorm.weight', 'transformer.h.16.post_attention_layernorm.bias', 'transformer.h.16.mlp.dense_h_to_4h.weight', 'transformer.h.16.mlp.dense_h_to_4h.bias', 'transformer.h.16.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.16.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.16.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.16.mlp.dense_4h_to_h.weight', 'transformer.h.16.mlp.dense_4h_to_h.bias', 'transformer.h.16.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.16.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.16.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.17.input_layernorm.weight', 'transformer.h.17.input_layernorm.bias', 'transformer.h.17.self_attention.query_key_value.weight', 'transformer.h.17.self_attention.query_key_value.bias', 'transformer.h.17.self_attention.query_key_value.weight.absmax', 'transformer.h.17.self_attention.query_key_value.weight.quant_map', 'transformer.h.17.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.17.self_attention.dense.weight', 'transformer.h.17.self_attention.dense.bias', 'transformer.h.17.self_attention.dense.weight.absmax', 'transformer.h.17.self_attention.dense.weight.quant_map', 'transformer.h.17.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.17.post_attention_layernorm.weight', 'transformer.h.17.post_attention_layernorm.bias', 'transformer.h.17.mlp.dense_h_to_4h.weight', 'transformer.h.17.mlp.dense_h_to_4h.bias', 'transformer.h.17.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.17.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.17.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.17.mlp.dense_4h_to_h.weight', 'transformer.h.17.mlp.dense_4h_to_h.bias', 'transformer.h.17.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.17.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.17.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.18.input_layernorm.weight', 'transformer.h.18.input_layernorm.bias', 'transformer.h.18.self_attention.query_key_value.weight', 'transformer.h.18.self_attention.query_key_value.bias', 'transformer.h.18.self_attention.query_key_value.weight.absmax', 'transformer.h.18.self_attention.query_key_value.weight.quant_map', 'transformer.h.18.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.18.self_attention.dense.weight', 'transformer.h.18.self_attention.dense.bias', 'transformer.h.18.self_attention.dense.weight.absmax', 'transformer.h.18.self_attention.dense.weight.quant_map', 'transformer.h.18.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.18.post_attention_layernorm.weight', 'transformer.h.18.post_attention_layernorm.bias', 'transformer.h.18.mlp.dense_h_to_4h.weight', 'transformer.h.18.mlp.dense_h_to_4h.bias', 'transformer.h.18.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.18.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.18.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.18.mlp.dense_4h_to_h.weight', 'transformer.h.18.mlp.dense_4h_to_h.bias', 'transformer.h.18.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.18.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.18.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.19.input_layernorm.weight', 'transformer.h.19.input_layernorm.bias', 'transformer.h.19.self_attention.query_key_value.weight', 'transformer.h.19.self_attention.query_key_value.bias', 'transformer.h.19.self_attention.query_key_value.weight.absmax', 'transformer.h.19.self_attention.query_key_value.weight.quant_map', 'transformer.h.19.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.19.self_attention.dense.weight', 'transformer.h.19.self_attention.dense.bias', 'transformer.h.19.self_attention.dense.weight.absmax', 'transformer.h.19.self_attention.dense.weight.quant_map', 'transformer.h.19.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.19.post_attention_layernorm.weight', 'transformer.h.19.post_attention_layernorm.bias', 'transformer.h.19.mlp.dense_h_to_4h.weight', 'transformer.h.19.mlp.dense_h_to_4h.bias', 'transformer.h.19.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.19.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.19.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.19.mlp.dense_4h_to_h.weight', 'transformer.h.19.mlp.dense_4h_to_h.bias', 'transformer.h.19.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.19.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.19.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.20.input_layernorm.weight', 'transformer.h.20.input_layernorm.bias', 'transformer.h.20.self_attention.query_key_value.weight', 'transformer.h.20.self_attention.query_key_value.bias', 'transformer.h.20.self_attention.query_key_value.weight.absmax', 'transformer.h.20.self_attention.query_key_value.weight.quant_map', 'transformer.h.20.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.20.self_attention.dense.weight', 'transformer.h.20.self_attention.dense.bias', 'transformer.h.20.self_attention.dense.weight.absmax', 'transformer.h.20.self_attention.dense.weight.quant_map', 'transformer.h.20.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.20.post_attention_layernorm.weight', 'transformer.h.20.post_attention_layernorm.bias', 'transformer.h.20.mlp.dense_h_to_4h.weight', 'transformer.h.20.mlp.dense_h_to_4h.bias', 'transformer.h.20.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.20.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.20.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.20.mlp.dense_4h_to_h.weight', 'transformer.h.20.mlp.dense_4h_to_h.bias', 'transformer.h.20.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.20.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.20.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.21.input_layernorm.weight', 'transformer.h.21.input_layernorm.bias', 'transformer.h.21.self_attention.query_key_value.weight', 'transformer.h.21.self_attention.query_key_value.bias', 'transformer.h.21.self_attention.query_key_value.weight.absmax', 'transformer.h.21.self_attention.query_key_value.weight.quant_map', 'transformer.h.21.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.21.self_attention.dense.weight', 'transformer.h.21.self_attention.dense.bias', 'transformer.h.21.self_attention.dense.weight.absmax', 'transformer.h.21.self_attention.dense.weight.quant_map', 'transformer.h.21.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.21.post_attention_layernorm.weight', 'transformer.h.21.post_attention_layernorm.bias', 'transformer.h.21.mlp.dense_h_to_4h.weight', 'transformer.h.21.mlp.dense_h_to_4h.bias', 'transformer.h.21.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.21.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.21.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.21.mlp.dense_4h_to_h.weight', 'transformer.h.21.mlp.dense_4h_to_h.bias', 'transformer.h.21.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.21.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.21.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.22.input_layernorm.weight', 'transformer.h.22.input_layernorm.bias', 'transformer.h.22.self_attention.query_key_value.weight', 'transformer.h.22.self_attention.query_key_value.bias', 'transformer.h.22.self_attention.query_key_value.weight.absmax', 'transformer.h.22.self_attention.query_key_value.weight.quant_map', 'transformer.h.22.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.22.self_attention.dense.weight', 'transformer.h.22.self_attention.dense.bias', 'transformer.h.22.self_attention.dense.weight.absmax', 'transformer.h.22.self_attention.dense.weight.quant_map', 'transformer.h.22.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.22.post_attention_layernorm.weight', 'transformer.h.22.post_attention_layernorm.bias', 'transformer.h.22.mlp.dense_h_to_4h.weight', 'transformer.h.22.mlp.dense_h_to_4h.bias', 'transformer.h.22.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.22.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.22.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.22.mlp.dense_4h_to_h.weight', 'transformer.h.22.mlp.dense_4h_to_h.bias', 'transformer.h.22.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.22.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.22.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.23.input_layernorm.weight', 'transformer.h.23.input_layernorm.bias', 'transformer.h.23.self_attention.query_key_value.weight', 'transformer.h.23.self_attention.query_key_value.bias', 'transformer.h.23.self_attention.query_key_value.weight.absmax', 'transformer.h.23.self_attention.query_key_value.weight.quant_map', 'transformer.h.23.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.23.self_attention.dense.weight', 'transformer.h.23.self_attention.dense.bias', 'transformer.h.23.self_attention.dense.weight.absmax', 'transformer.h.23.self_attention.dense.weight.quant_map', 'transformer.h.23.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.23.post_attention_layernorm.weight', 'transformer.h.23.post_attention_layernorm.bias', 'transformer.h.23.mlp.dense_h_to_4h.weight', 'transformer.h.23.mlp.dense_h_to_4h.bias', 'transformer.h.23.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.23.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.23.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.23.mlp.dense_4h_to_h.weight', 'transformer.h.23.mlp.dense_4h_to_h.bias', 'transformer.h.23.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.23.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.23.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.24.input_layernorm.weight', 'transformer.h.24.input_layernorm.bias', 'transformer.h.24.self_attention.query_key_value.weight', 'transformer.h.24.self_attention.query_key_value.bias', 'transformer.h.24.self_attention.query_key_value.weight.absmax', 'transformer.h.24.self_attention.query_key_value.weight.quant_map', 'transformer.h.24.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.24.self_attention.dense.weight', 'transformer.h.24.self_attention.dense.bias', 'transformer.h.24.self_attention.dense.weight.absmax', 'transformer.h.24.self_attention.dense.weight.quant_map', 'transformer.h.24.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.24.post_attention_layernorm.weight', 'transformer.h.24.post_attention_layernorm.bias', 'transformer.h.24.mlp.dense_h_to_4h.weight', 'transformer.h.24.mlp.dense_h_to_4h.bias', 'transformer.h.24.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.24.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.24.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.24.mlp.dense_4h_to_h.weight', 'transformer.h.24.mlp.dense_4h_to_h.bias', 'transformer.h.24.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.24.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.24.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.25.input_layernorm.weight', 'transformer.h.25.input_layernorm.bias', 'transformer.h.25.self_attention.query_key_value.weight', 'transformer.h.25.self_attention.query_key_value.bias', 'transformer.h.25.self_attention.query_key_value.weight.absmax', 'transformer.h.25.self_attention.query_key_value.weight.quant_map', 'transformer.h.25.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.25.self_attention.dense.weight', 'transformer.h.25.self_attention.dense.bias', 'transformer.h.25.self_attention.dense.weight.absmax', 'transformer.h.25.self_attention.dense.weight.quant_map', 'transformer.h.25.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.25.post_attention_layernorm.weight', 'transformer.h.25.post_attention_layernorm.bias', 'transformer.h.25.mlp.dense_h_to_4h.weight', 'transformer.h.25.mlp.dense_h_to_4h.bias', 'transformer.h.25.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.25.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.25.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.25.mlp.dense_4h_to_h.weight', 'transformer.h.25.mlp.dense_4h_to_h.bias', 'transformer.h.25.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.25.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.25.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.26.input_layernorm.weight', 'transformer.h.26.input_layernorm.bias', 'transformer.h.26.self_attention.query_key_value.weight', 'transformer.h.26.self_attention.query_key_value.bias', 'transformer.h.26.self_attention.query_key_value.weight.absmax', 'transformer.h.26.self_attention.query_key_value.weight.quant_map', 'transformer.h.26.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.26.self_attention.dense.weight', 'transformer.h.26.self_attention.dense.bias', 'transformer.h.26.self_attention.dense.weight.absmax', 'transformer.h.26.self_attention.dense.weight.quant_map', 'transformer.h.26.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.26.post_attention_layernorm.weight', 'transformer.h.26.post_attention_layernorm.bias', 'transformer.h.26.mlp.dense_h_to_4h.weight', 'transformer.h.26.mlp.dense_h_to_4h.bias', 'transformer.h.26.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.26.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.26.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.26.mlp.dense_4h_to_h.weight', 'transformer.h.26.mlp.dense_4h_to_h.bias', 'transformer.h.26.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.26.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.26.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.27.input_layernorm.weight', 'transformer.h.27.input_layernorm.bias', 'transformer.h.27.self_attention.query_key_value.weight', 'transformer.h.27.self_attention.query_key_value.bias', 'transformer.h.27.self_attention.query_key_value.weight.absmax', 'transformer.h.27.self_attention.query_key_value.weight.quant_map', 'transformer.h.27.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.27.self_attention.dense.weight', 'transformer.h.27.self_attention.dense.bias', 'transformer.h.27.self_attention.dense.weight.absmax', 'transformer.h.27.self_attention.dense.weight.quant_map', 'transformer.h.27.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.27.post_attention_layernorm.weight', 'transformer.h.27.post_attention_layernorm.bias', 'transformer.h.27.mlp.dense_h_to_4h.weight', 'transformer.h.27.mlp.dense_h_to_4h.bias', 'transformer.h.27.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.27.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.27.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.27.mlp.dense_4h_to_h.weight', 'transformer.h.27.mlp.dense_4h_to_h.bias', 'transformer.h.27.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.27.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.27.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.28.input_layernorm.weight', 'transformer.h.28.input_layernorm.bias', 'transformer.h.28.self_attention.query_key_value.weight', 'transformer.h.28.self_attention.query_key_value.bias', 'transformer.h.28.self_attention.query_key_value.weight.absmax', 'transformer.h.28.self_attention.query_key_value.weight.quant_map', 'transformer.h.28.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.28.self_attention.dense.weight', 'transformer.h.28.self_attention.dense.bias', 'transformer.h.28.self_attention.dense.weight.absmax', 'transformer.h.28.self_attention.dense.weight.quant_map', 'transformer.h.28.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.28.post_attention_layernorm.weight', 'transformer.h.28.post_attention_layernorm.bias', 'transformer.h.28.mlp.dense_h_to_4h.weight', 'transformer.h.28.mlp.dense_h_to_4h.bias', 'transformer.h.28.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.28.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.28.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.28.mlp.dense_4h_to_h.weight', 'transformer.h.28.mlp.dense_4h_to_h.bias', 'transformer.h.28.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.28.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.28.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.29.input_layernorm.weight', 'transformer.h.29.input_layernorm.bias', 'transformer.h.29.self_attention.query_key_value.weight', 'transformer.h.29.self_attention.query_key_value.bias', 'transformer.h.29.self_attention.query_key_value.weight.absmax', 'transformer.h.29.self_attention.query_key_value.weight.quant_map', 'transformer.h.29.self_attention.query_key_value.weight.quant_state.bitsandbytes__nf4', 'transformer.h.29.self_attention.dense.weight', 'transformer.h.29.self_attention.dense.bias', 'transformer.h.29.self_attention.dense.weight.absmax', 'transformer.h.29.self_attention.dense.weight.quant_map', 'transformer.h.29.self_attention.dense.weight.quant_state.bitsandbytes__nf4', 'transformer.h.29.post_attention_layernorm.weight', 'transformer.h.29.post_attention_layernorm.bias', 'transformer.h.29.mlp.dense_h_to_4h.weight', 'transformer.h.29.mlp.dense_h_to_4h.bias', 'transformer.h.29.mlp.dense_h_to_4h.weight.absmax', 'transformer.h.29.mlp.dense_h_to_4h.weight.quant_map', 'transformer.h.29.mlp.dense_h_to_4h.weight.quant_state.bitsandbytes__nf4', 'transformer.h.29.mlp.dense_4h_to_h.weight', 'transformer.h.29.mlp.dense_4h_to_h.bias', 'transformer.h.29.mlp.dense_4h_to_h.weight.absmax', 'transformer.h.29.mlp.dense_4h_to_h.weight.quant_map', 'transformer.h.29.mlp.dense_4h_to_h.weight.quant_state.bitsandbytes__nf4', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training arguments for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./results\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 10\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 20\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFTTrainer Class\n",
    "\n",
    "`SFTTrainer` is a wrapper around the `transformers.Trainer` class, designed specifically for supervised fine-tuning (SFT). It handles proper initialization of the `PeftModel` when a `PeftConfig` object is passed.\n",
    "\n",
    "#### Args:\n",
    "- **model** (`Union[transformers.PreTrainedModel, nn.Module, str]`):  \n",
    "  The model to train, which can be a `PreTrainedModel`, a `torch.nn.Module`, or a string specifying the model name to load from cache or download. If a `PeftConfig` is provided, the model is converted to a `PeftModel`.\n",
    "\n",
    "- **args** (`Optional[SFTConfig]`):  \n",
    "  Configuration arguments for the trainer. Defaults to a basic instance of `SFTConfig` with `output_dir` set to a temporary directory (`tmp_trainer`) if not provided.\n",
    "\n",
    "- **data_collator** (`Optional[transformers.DataCollator]`):  \n",
    "  Data collator to use during training.\n",
    "\n",
    "- **train_dataset** (`Optional[datasets.Dataset]`):  \n",
    "  The dataset used for training. It is recommended to use `trl.trainer.ConstantLengthDataset` for dataset creation.\n",
    "\n",
    "- **eval_dataset** (`Optional[Union[datasets.Dataset, Dict[str, datasets.Dataset]]]`):  \n",
    "  Dataset used for evaluation. As with the training dataset, it is recommended to use `trl.trainer.ConstantLengthDataset`.\n",
    "\n",
    "- **tokenizer** (`Optional[transformers.PreTrainedTokenizer]`):  \n",
    "  Tokenizer for the training process. If not specified, the model's associated tokenizer will be used.\n",
    "\n",
    "- **model_init** (`Callable[[], transformers.PreTrainedModel]`):  \n",
    "  A function for model initialization. If none is provided, the default initializer will be used.\n",
    "\n",
    "- **compute_metrics** (`Callable[[transformers.EvalPrediction], Dict]`, optional):  \n",
    "  Function to compute metrics during evaluation. Should return a dictionary mapping metric names to values. If not provided, only the loss is computed during evaluation.\n",
    "\n",
    "- **callbacks** (`List[transformers.TrainerCallback]`):  \n",
    "  A list of callbacks to be used during training.\n",
    "\n",
    "- **optimizers** (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):  \n",
    "  The optimizer and learning rate scheduler for the training process.\n",
    "\n",
    "- **preprocess_logits_for_metrics** (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):  \n",
    "  A function to preprocess the logits before computing metrics.\n",
    "\n",
    "- **peft_config** (`Optional[PeftConfig]`):  \n",
    "  The `PeftConfig` object used to initialize the `PeftModel`.\n",
    "\n",
    "- **formatting_func** (`Optional[Callable]`):  \n",
    "  Function used to format data for creating the `ConstantLengthDataset`.\n",
    "\n",
    "#### Inherits:\n",
    "- **`transformers.Trainer`**: Inherits all attributes and methods from the `Trainer` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.integrations import MLflowCallback, WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa07261991e7460ab1274bb6042636c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cb in trainer.callback_handler.callbacks:\n",
    "    if isinstance(cb, MLflowCallback) or isinstance(cb, WandbCallback):\n",
    "        trainer.callback_handler.remove_callback(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DISABLE_MLFLOW_INTEGRATION\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 04:06, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.948900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.864000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=1.9064565658569337, metrics={'train_runtime': 259.7225, 'train_samples_per_second': 1.232, 'train_steps_per_second': 0.077, 'total_flos': 2159489078968320.0, 'train_loss': 1.9064565658569337, 'epoch': 0.03249390739236393})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is a large language model? [/INST] A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '<s>[INST] What is a large language model? [/INST] A\\n'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
