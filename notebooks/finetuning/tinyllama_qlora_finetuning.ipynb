{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/Abi_GenAI_Sessions'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/Abi_GenAI_Sessions\"\n",
    ")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a json file from .kaggle and set the environment variables\n",
    "# os.environ['KAGGLE_USERNAME'] = 'your_kaggle_username'\n",
    "# os.environ['KAGGLE_KEY'] = 'your_kaggle_api_key'\n",
    "\n",
    "with open(\".config/kaggle.json\") as f:\n",
    "    kaggle_json = json.load(f)\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = kaggle_json[\"username\"]\n",
    "os.environ[\"KAGGLE_KEY\"] = kaggle_json[\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./data/taylor-swift-all-lyrics-30-albums.zip\"):\n",
    "    kaggle.api.dataset_download_files(\n",
    "        \"ishikajohari/taylor-swift-all-lyrics-30-albums\",\n",
    "        path=\"./data\",\n",
    "        unzip=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 10:32:48.900841: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 10:32:48.900889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 10:32:48.901780: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 10:32:48.908044: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 10:32:49.623287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/data/Albums\"\n",
    "lyrics_files = glob.glob(f\"{data_path}/**/*.txt\", recursive=True)\n",
    "print(len(lyrics_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_join_txt_files(file_paths):\n",
    "    contents = []\n",
    "    for path in file_paths:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            contents.append(file.read())\n",
    "    return \"\\n\\n\".join(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_content = read_and_join_txt_files(lyrics_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 ContributorsTranslationsEspañolPortuguêsFrançaisClean Lyrics[Verse 1]\n",
      "The drought was the very worst (Oh-oh, oh-oh)\n",
      "When the flowers that we'd grown together died of thirst\n",
      "It was months and months of back and forth (Oh-oh, oh-oh)\n",
      "You're still all over me\n",
      "Like a wine-stained dress I can't wear anymore\n",
      "\n",
      "[Pre-Chorus]\n",
      "Hung my head as I lost the war\n",
      "And the sky turned black like a perfect storm\n",
      "[Chorus]\n",
      "Rain came pouring down\n",
      "When I was drowning, that's when I could finally breathe\n",
      "And by morning\n",
      "Gone was any trace of you, I think I am finally clean\n",
      "(Oh, oh, oh, oh)\n",
      "\n",
      "[Verse 2]\n",
      "There was nothing left to do (Oh-oh, oh-oh)\n",
      "When the butterflies turned to\n",
      "Dust that covered my whole room\n",
      "So I punched a hole in the roof (Oh-oh, oh-oh)\n",
      "Let the flood carry away all my pictures of you\n",
      "\n",
      "[Pre-Chorus]\n",
      "The water filled my lungs, I screamed so loud\n",
      "But no one heard a thing\n",
      "\n",
      "[Chorus]\n",
      "Rain came pouring down\n",
      "When I was drowning, that's when I could finally breathe\n",
      "And by morning\n",
      "Gone was any trace of yo\n"
     ]
    }
   ],
   "source": [
    "print(joined_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !#$&'*,./0123456789?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz¦ÃÅÇÜáâãåçêñöüăčšΕάήαεηικλμνρστφМРСУагдийклнопрсуьїابةتجرسعفلمهيیदनहिी्ทปภยลษาแไᜄᜆᜎᜓ᜔ếệ‌‎“†…中文日本翻語译국어한\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the file by removing/replacing unnecessary characters and removing sections\n",
    "# that are not lyrics\n",
    "replace_with_space = [\"\\u2005\", \"\\u200b\", \"\\u205f\", \"\\xa0\", \"-\"]\n",
    "replace_letters = {\n",
    "    \"í\": \"i\",\n",
    "    \"é\": \"e\",\n",
    "    \"ï\": \"i\",\n",
    "    \"ó\": \"o\",\n",
    "    \";\": \",\",\n",
    "    \"‘\": \"'\",\n",
    "    \"’\": \"'\",\n",
    "    \":\": \",\",\n",
    "    \"е\": \"e\",\n",
    "}\n",
    "remove_list = [\"\\)\", \"\\(\", \"–\", '\"', \"”\", '\"', \"\\[.*\\]\", \".*\\|.*\", \"—\"]\n",
    "\n",
    "cleaned_lyrics = joined_content\n",
    "\n",
    "for old, new in replace_letters.items():\n",
    "    cleaned_lyrics = cleaned_lyrics.replace(old, new)\n",
    "for string in remove_list:\n",
    "    cleaned_lyrics = re.sub(string, \"\", cleaned_lyrics)\n",
    "for string in replace_with_space:\n",
    "    cleaned_lyrics = re.sub(string, \" \", cleaned_lyrics)\n",
    "print(\"\".join(sorted(set(cleaned_lyrics))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3730\n"
     ]
    }
   ],
   "source": [
    "# Setting aside a portion for training the model and a portion for testing the data to prevent\n",
    "# the model from overfitting to the data it is tested on\n",
    "split_point = int(len(cleaned_lyrics) * 0.95)\n",
    "train_data = cleaned_lyrics[:split_point]\n",
    "test_data = cleaned_lyrics[split_point:]\n",
    "train_data_seg = []\n",
    "for i in range(0, len(train_data), 500):\n",
    "    text = train_data[i : min(i + 500, len(train_data))]\n",
    "    train_data_seg.append(text)\n",
    "train_data_seg = Dataset.from_dict({\"text\": train_data_seg})\n",
    "print(len(train_data_seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506b11e4b62d4e0eb4976d6a62ae05f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model with double quantization\n",
    "model_name = \"PY007/TinyLlama-1.1B-step-50K-105b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model with double quantization and 4-bit quantization for the model weights and activations\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model with the quantization configuration and setting the device map to auto\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creating tokenizer and defining the pad token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating lyrics with the base model. The repetition penalty in the generation config prevents the model from continually repeating the same string.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def generate_lyrics(query, model):\n",
    "    encoding = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=250,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids, generation_config=generation_config\n",
    "    )\n",
    "    text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"INPUT\\n\", query, \"\\n\\nOUTPUT\\n\", text_output[len(query) :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT\n",
      " , so take me to Florida\n",
      "I've got some regrets, I'll bury them in Florida\n",
      "Tell me I'm despicable, say it's unforgivable\n",
      "What a crash, what a rush, fuck me up, Florida\n",
      "It's one hell of a drug\n",
      "It's one hell of a drug\n",
      "Love left me like this and I don't want to exist\n",
      "So take me to Florida\n",
      "\n",
      "Little did you know your home's really only\n",
      "A town you're just a guest in Florida\n",
      "So you work your life away just to pay\n",
      "For a time share down in Destin Take me to Florida\n",
      "Little did you know your home's really onl \n",
      "\n",
      "OUTPUT\n",
      " \n",
      "You can get the best of everything\n",
      "But if you do not have enough money for rent\n",
      "Then you are going to be living out there\n",
      "And that is where we all live now\n",
      "We all live here together\n",
      "\n",
      "Now let us go back to our first song\n",
      "The second song was called \"Florida\"\n",
      "This is my favorite song from the album\n",
      "My favorite song from the album\n",
      "\n",
      "Oh yeah, oh yeah, oh yeah\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well, well\n",
      "Well, well, well\n"
     ]
    }
   ],
   "source": [
    "generate_lyrics(test_data[200:700], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting arguments for low-rank adaptation \n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_alpha = 32 # The weight matrix is scaled by lora_alpha/lora_rank, so I set lora_alpha = lora_rank to remove scaling\n",
    "lora_dropout = 0.05 \n",
    "lora_rank = 32 \n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_rank,\n",
    "    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n",
    "    task_type=\"CAUSAL_LM\")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting training arguments\n",
    "\n",
    "output_dir = \"tommyadams/tinyllama\"  # Model repo on your hugging face account where you want to save your model\n",
    "per_device_train_batch_size = 3\n",
    "gradient_accumulation_steps = 2\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_strategy = \"steps\"\n",
    "save_steps = 10\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-3\n",
    "max_grad_norm = 0.3  # Sets limit for gradient clipping\n",
    "max_steps = 50  # Number of training steps\n",
    "warmup_ratio = 0.03  # Portion of steps used for learning_rate to warmup from 0\n",
    "lr_scheduler_type = \"cosine\"  # I chose cosine to avoid learning plateaus\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b427f3b0bd2c4bf59aa2924ce27d3c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_data_seg,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=500,\n",
    "    dataset_text_field='text',\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments\n",
    ")\n",
    "peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 02:37, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.849100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.752500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.703500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/finetune_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=2.778901176452637, metrics={'train_runtime': 160.7288, 'train_samples_per_second': 1.866, 'train_steps_per_second': 0.311, 'total_flos': 320408209833984.0, 'train_loss': 2.778901176452637, 'epoch': 0.08038585209003216})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_lyrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_lyrics\u001b[49m(test_data[\u001b[38;5;241m200\u001b[39m:\u001b[38;5;241m300\u001b[39m], peft_model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_lyrics' is not defined"
     ]
    }
   ],
   "source": [
    "generate_lyrics(test_data[200:300], peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################### END ######################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
